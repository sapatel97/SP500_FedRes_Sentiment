---
title: "Web_Scraping"
author: "David Barnes"
date: "2/15/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}
library(dplyr)
library(rvest)
#library(doParallel)
library(foreach)
library(textstem)
library(tm)
library(readr)
library(httr)
library(tidytext)
#library(doSNOW)
```

```{r eval = FALSE}
getStockData <- function(stockTicker) {
  # API KEY
  avKey <- "REDACTED"
  baseLink <- "https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&outputsize=full&symbol="

    avLinkGenerator <- paste(baseLink, 
                              stockTicker, "&interval=60min", "&datatype=csv",
                              "&apikey=", avKey, sep = "")
    
  avRequest <- GET(avLinkGenerator)
  avParsed <- read.delim(text = httr::content(avRequest, as = "text"), sep = ",")
  avParsed$timestamp <- as.Date(avParsed$timestamp)
  return(avParsed)
}

SPY_data <- getStockData("SPY")
save(SPY_data, file = "./data/SPY_Data.Rdata")
```


```{r eval = FALSE}
speeches_get <- GET("https://www.federalreserve.gov/json/ne-speeches.json")
```

```{r eval = FALSE}
# cl <- makeCluster(detectCores() - 1)
# 
# registerDoSNOW(cl)
# 
# iterations <- 857
# pb <- txtProgressBar(max = iterations, style = 3)
# 
# progress <- function(n) setTxtProgressBar(pb, n)
# opts <- list(progress = progress)
# 
# t1 <- proc.time()

speeches <- httr::content(speeches_get) %>%
  bind_rows() %>%
  transmute(Name = t,
            url = stringr::str_replace(l, "//", "/"), #rid of the "//" at the beginning of the url
            url = paste0("https://www.federalreserve.gov/", url)) %>%
  filter(!is.na(Name)) # filtering NA as the last row of the json is not a speech


speeches$speech_transcript <- "" #init blank column for the next loops
speeches$date <- ""


for(i in 1:nrow(speeches)) {
  
  html_temp <- read_html(speeches[i,]$url)
  # get the speech transcripts
    speeches[i,]$speech_transcript <- html_temp %>%
      html_node("#content") %>%
      html_node("#article") %>%
      html_node("div:nth-child(3)") %>%
      html_text() %>%
      stringr::str_squish() # getting rid of multiple spaces and stuff
    
  # get the dates of the speeches
    speeches[i,]$date <- html_temp %>%
      html_node("#content") %>%
      html_node("p") %>%
      html_text()
    # speeches[i,]$date <- read_html(speeches[i,]$url) %>%
    #   html_node("#content") %>%
    #   html_node("p") %>%
    #   html_text()
  # get the speaker
    speeches$Name[i] <- html_temp %>%
      html_node("#content") %>%
      html_node("#article") %>%
      html_nodes("p") %>%
      html_text() %>%
      `[[`(2)
    # speeches$Name[i] <- read_html(speeches[i,]$url,) %>% 
    #   html_node("#content") %>%
    #   html_node("#article") %>%
    #   html_nodes("p") %>%
    #   html_text() %>%
    #   `[[`(2)
    
    print(i)
    print(speeches$Name[i])
    print(speeches$date[i])
  
}
 
proc.time() - t1

close(pb)
stopCluster(cl)

# save(speeches, file = "./data/Speeches_Data.Rdata")
# speech_names <- data.frame(speeches$Name)
# save(speech_names, file = "./data/Speech_Names.Rdata")
```

```{r eval = FALSE}
# cl <- makeCluster(detectCores() - 1)
# 
# registerDoParallel(cl)
# 
# t1 <- proc.time()
# # grab speeches
# speeches <- content(speeches_get) %>%
#   bind_rows() %>%
#   transmute(Name = t,
#             url = str_replace(l, "//", "/"), #rid of the "//" at the beginning of the url
#             url = paste0("https://www.federalreserve.gov/", url)) %>%
#   filter(!is.na(Name)) # filtering NA as the last row of the json is not a speech
# speeches$speech_transcript <- "" # making sure the column speech_transcript works before I try to assign its values in the loop
# 
# speeches <- speeches[1:100,]
# 
# # loop through the speeches
# speech_parallel <- foreach(i = 1:nrow(speeches), .packages = c("tidyverse", "httr", "rvest")) %dopar% { # going through urls and getting the text of the speeches
#   speeches[i,]$speech_transcript <- read_html(speeches[i,]$url) %>%
#     html_node("#content") %>%
#     html_node("#article") %>%
#     html_node("div:nth-child(3)") %>%
#     html_text() %>%
#     str_squish() # getting rid of multiple spaces and stuff
# }
# 
# df <- data.frame(matrix(unlist(speech_parallel), nrow=length(speech_parallel), byrow=TRUE))
# 
# for(i in speech_parallel) {
#   speeches$speech_transcript[i,] <- data.frame(speech_parallel[[i]])
# }
# 
# 
# 
# proc.time() - t1  
# 
# stopCluster(cl)
```

```{r}
# load("./Speeches.RData")
load("./data/Speeches_Data.Rdata")
```


All 848 speeches from federal reserve. Probably shouldn't have done this much as it was really hard on my computer, but it was too tempting (sentiment analysis vs SP500 in future?).

```{r}
# example1 <-speeches$speech_transcript[1]
# example1 #full speech
speech_text <- speeches$speech_transcript
```

```{r Text Cleaning}
# remove doubleback slashes and single backslashes
speech_text <- gsub("\\\\", "", speech_text)
speech_text <- gsub("\"", "", speech_text)
# remove parenthesis and their contents
speech_text <- gsub("\\s*\\([^\\)]+\\)", "", speech_text)
# remove hyperlinks until a space is found
speech_text <- gsub(" ?(f|ht)(tp)s?(://)(\\S*)[./](\\S*)", "", speech_text)
# remove small strings/words that are in the text
speech_text <- gsub("pp.", "", speech_text)
speech_text <- gsub("vol.", "", speech_text)
# remove punctuation and various symbols
speech_text <- tm::removePunctuation(speech_text)
speech_text <- gsub("–", "", speech_text)
speech_text <- stripWhitespace(speech_text)
# remove stopwords
speech_text <- removeWords(speech_text, stopwords("en"))
speech_text <- tolower(speech_text)
# remove "return text" string which is prevalent in many speeches
speech_text <- gsub("return text", "", speech_text)
# remove this special dash from speeches
speech_text <- gsub(" — ", "", speech_text)
# remove numbers
speech_text <- removeNumbers(speech_text)


head(speech_text, n = 1)
# looks pretty good now, lets lemmatize
```

```{r}
speech_text <- lemmatize_strings(speech_text)
head(speech_text, n = 1)
```

```{r}
speeches$rowindex <- rownames(speeches)

speech_text <- data.frame(speech_text)
colnames(speech_text) <- "calltext"

tokens <- speech_text %>%
  group_by(rownames(speech_text)) %>%
  unnest_tokens(tbl = ., output = word, input = calltext)

sent_tokens <- tokens %>%
  inner_join(get_sentiments("loughran")) %>%
  count(sentiment) %>%
  tidyr::pivot_wider(values_from = n, names_from = sentiment, values_fill = 0) %>%
  mutate(sentiment = positive - negative)

names(sent_tokens)[names(sent_tokens) == 'rownames(speech_text)'] <- 'rowindex'

speeches_sent <- left_join(speeches, sent_tokens, by.x = 'rowindex', by.y = "rowindex") 

head(speeches_sent$date)

speeches_sent$date <- as.Date(speeches_sent$date, format = "%b %d, %Y")

head(speeches_sent$date)

speeches_sent$sentiment <- as.numeric(speeches_sent$sentiment)

speeches_sent$speech_transcript <- ""

save(speeches_sent, file = "./data/Speeches_Sent.Rdata")
```

```{r}
# Aggregate the speeches with sent dataframe

load("./data/Speeches_Sent.Rdata")

speaker_occurences <- as.data.frame(table(speeches_sent$Name))
speaker_occurences <- speaker_occurences[order(-speaker_occurences$Freq),]
head(speaker_occurences)



speeches_agg_sent <- speeches_sent %>%
  group_by(date) %>%
  select(date, sentiment) %>%
  mutate_each(funs(replace(sentiment, which(is.na(.)), 1))) %>% # NA values <- 0 in sentiment column
  summarise(sentiment = mean(sentiment))

names_agg_sent <- speeches_sent %>%
  group_by(Name) %>%
  select(Name, date, sentiment) %>%
  mutate_each(funs(replace(sentiment, which(is.na(.)), 1))) %>% # NA values <- 0 in sentiment column
  summarise(sentiment = mean(sentiment))

names_date_agg_sent <- speeches_sent %>%
  group_by(date, Name) %>%
  select(Name, date, sentiment) %>%
  mutate_each(funs(replace(sentiment, which(is.na(.)), 1))) %>% # NA values <- 0 in sentiment column
  summarise(sentiment = mean(sentiment))
```

